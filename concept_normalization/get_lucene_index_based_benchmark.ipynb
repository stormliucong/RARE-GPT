{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from whoosh import scoring\n",
    "from whoosh.index import create_in, open_dir\n",
    "from whoosh.fields import Schema, TEXT, ID\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "from whoosh.index import open_dir\n",
    "from whoosh.qparser import QueryParser, OrGroup, AndGroup\n",
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hp_synonyms():\n",
    "    # load hpo from json\n",
    "    hpo_json = json.load(open('hp.json'))\n",
    "    synonym_dict_list = []\n",
    "    nodes = hpo_json['graphs'][0]['nodes']\n",
    "    for node in nodes:\n",
    "        # \"id\" : \"http://purl.obolibrary.org/obo/HP_0000016\"\n",
    "        try:\n",
    "            id_component_list = node['id'].split('/')\n",
    "            if 'HP_' in id_component_list[-1]:\n",
    "                synonym_dict = {}\n",
    "                synonym_dict['hp_id'] = id_component_list[-1]\n",
    "                synonym_dict['name'] = node['lbl']\n",
    "                synonym_dict['synonyms'] = [node['lbl']]\n",
    "                if 'meta' in node:\n",
    "                    if 'synonyms' in node['meta']:\n",
    "                        synonyms = node['meta']['synonyms']\n",
    "                        for synonym in synonyms:\n",
    "                            synonym_dict['synonyms'].append(synonym['val'])\n",
    "                synonym_dict_list.append(synonym_dict)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "    return synonym_dict_list\n",
    "\n",
    "def create_index(index_dir, synonym_dict_list):\n",
    "    # Create an index and schema\n",
    "    stem_ana = StemmingAnalyzer()\n",
    "    custom_schema = Schema(hp_id=ID(stored=True),\n",
    "                hp_desc=TEXT(stored=True, analyzer=stem_ana),\n",
    "    )\n",
    "    # create if not exist\n",
    "    if not os.path.exists(index_dir):\n",
    "        os.mkdir(index_dir)\n",
    "    else:\n",
    "        # delete forecely and recreate\n",
    "        import shutil\n",
    "        shutil.rmtree(index_dir)\n",
    "        os.mkdir(index_dir)\n",
    "    index = create_in(index_dir, custom_schema)\n",
    "\n",
    "    # Open the index\n",
    "    index = open_dir(index_dir)\n",
    "\n",
    "    # Create a writer to add documents to the index\n",
    "    writer = index.writer()\n",
    "\n",
    "    # Add documents to the index\n",
    "    for i in tqdm(range(len(synonym_dict_list))):\n",
    "        phrase = synonym_dict_list[i]\n",
    "        synonyms = phrase['synonyms']\n",
    "        # synonyms.extend([phrase['name']])\n",
    "        for synonym in synonyms:\n",
    "            writer.add_document(hp_id=str(phrase['hp_id']),\n",
    "                                hp_desc=synonym\n",
    "                                )\n",
    "    writer.commit()\n",
    "    return 0\n",
    "\n",
    "def query_terms(index_dir, query_name_list, output, top_k=1):\n",
    "    def custom_scoring(searcher, fieldname, text, matcher):\n",
    "        # frequency = scoring.Frequency().scorer(searcher, fieldname, text).score(matcher)\n",
    "        # tfidf =  scoring.TF_IDF().scorer(searcher, fieldname, text).score(matcher)\n",
    "        bm25 = scoring.BM25F().scorer(searcher, fieldname, text).score(matcher)\n",
    "        return bm25\n",
    "    \n",
    "    query_results = []\n",
    "    index = open_dir(index_dir)\n",
    "    my_weighting = scoring.FunctionWeighting(custom_scoring)\n",
    "    # scoring.BM25F(B=10, K1=0.1)\n",
    "    searcher = index.searcher(weighting=scoring.BM25F(B=10, K1=0.1))\n",
    "    query_parser = QueryParser('hp_desc', schema=index.schema, group=OrGroup)\n",
    "    # Tokenize the query text using the same analyzer\n",
    "    for query_name in tqdm(query_name_list):\n",
    "        query = query_parser.parse('{}'.format(query_name))\n",
    "        results = searcher.search(query, limit=1, scored=False)\n",
    "        # Retrieve the top k matching phrase\n",
    "        if len(results) > 0:\n",
    "            # Retrieve and print the ranked matching phrases\n",
    "            # top k only\n",
    "            results = results[:top_k]\n",
    "            for i, result in enumerate(results):\n",
    "                hp_desc = result['hp_desc']\n",
    "                hp_id = result['hp_id']\n",
    "                score = result.score\n",
    "                query_results.append({'query_name': query_name, 'hp_desc': hp_desc, 'hp_id': hp_id, 'score': score})\n",
    "        else:\n",
    "            query_results.append({'query_name': query_name, 'hp_desc': '', 'hp_id': '', 'score': -1})\n",
    "    searcher.close()\n",
    "    query_results_df = pd.DataFrame(query_results)\n",
    "    query_results_df.to_csv(output, index=False)   \n",
    "    return query_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18263/18263 [00:05<00:00, 3596.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonym_dict_list = load_hp_synonyms()\n",
    "index_dir = './woosh_index'\n",
    "create_index(index_dir, synonym_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/498 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:15<00:00, 32.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_name</th>\n",
       "      <th>hp_desc</th>\n",
       "      <th>hp_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Liver fibrosis</td>\n",
       "      <td>Liver fibrosis</td>\n",
       "      <td>HP_0001395</td>\n",
       "      <td>23.713547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hepatic fibrosis</td>\n",
       "      <td>Hepatic fibrosis</td>\n",
       "      <td>HP_0001395</td>\n",
       "      <td>24.282408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Portal tract fibrosis</td>\n",
       "      <td>Portal fibrosis</td>\n",
       "      <td>HP_0006580</td>\n",
       "      <td>26.451651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fibrosis of the liver</td>\n",
       "      <td>Liver fibrosis</td>\n",
       "      <td>HP_0001395</td>\n",
       "      <td>23.713547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fibrotic liver disease</td>\n",
       "      <td>Liver disease</td>\n",
       "      <td>HP_0001392</td>\n",
       "      <td>20.715952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>Bovine milk hypersensitivity</td>\n",
       "      <td>Hypersensitivity</td>\n",
       "      <td>HP_0041092</td>\n",
       "      <td>24.677188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>Dairy allergy</td>\n",
       "      <td>Allergy to dairy</td>\n",
       "      <td>HP_0410327</td>\n",
       "      <td>25.211736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Allergy to cow's milk</td>\n",
       "      <td>Cow milk allergy</td>\n",
       "      <td>HP_0100327</td>\n",
       "      <td>27.288984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>Milk protein allergy</td>\n",
       "      <td>Milk allergy</td>\n",
       "      <td>HP_0100327</td>\n",
       "      <td>23.474081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Lactose intolerance</td>\n",
       "      <td>Lactose intolerance</td>\n",
       "      <td>HP_0004789</td>\n",
       "      <td>29.155194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>498 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       query_name              hp_desc       hp_id      score\n",
       "0                  Liver fibrosis       Liver fibrosis  HP_0001395  23.713547\n",
       "1                Hepatic fibrosis     Hepatic fibrosis  HP_0001395  24.282408\n",
       "2           Portal tract fibrosis      Portal fibrosis  HP_0006580  26.451651\n",
       "3           Fibrosis of the liver       Liver fibrosis  HP_0001395  23.713547\n",
       "4          Fibrotic liver disease        Liver disease  HP_0001392  20.715952\n",
       "..                            ...                  ...         ...        ...\n",
       "493  Bovine milk hypersensitivity     Hypersensitivity  HP_0041092  24.677188\n",
       "494                 Dairy allergy     Allergy to dairy  HP_0410327  25.211736\n",
       "495         Allergy to cow's milk     Cow milk allergy  HP_0100327  27.288984\n",
       "496          Milk protein allergy         Milk allergy  HP_0100327  23.474081\n",
       "497           Lactose intolerance  Lactose intolerance  HP_0004789  29.155194\n",
       "\n",
       "[498 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_name_list = pd.read_csv('synonym_df.csv')['synonym'].tolist()\n",
    "output = './woosh_query_results.csv'\n",
    "query_terms(index_dir, query_name_list, output, top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31536/31536 [03:20<00:00, 157.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07302301984907096"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# typos evaluation\n",
    "query_name_list = pd.read_csv('hp_typos_list.csv')['typo'].tolist()\n",
    "output = './woosh_typos_query_results.csv'\n",
    "index_dir = './woosh_index'\n",
    "query_results = query_terms(index_dir, query_name_list, output, top_k=1)\n",
    "evaluation_df = query_results.merge(pd.read_csv('hp_typos_list.csv'),left_on='query_name',right_on='typo')\n",
    "evaluation_df['correct_match'] = evaluation_df['hp_id_x'] == evaluation_df['hp_id_y']\n",
    "# rate of correct_match\n",
    "evaluation_df['correct_match'].sum()/evaluation_df['correct_match'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/498 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:15<00:00, 31.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2891566265060241"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpt synonyms evaluation\n",
    "query_name_list = pd.read_csv('gpt_synonym_df.csv')['synonym'].tolist()\n",
    "output = './woosh_gpt_synonyms_query_results.csv'\n",
    "index_dir = './woosh_index'\n",
    "query_results = query_terms(index_dir, query_name_list, output, top_k=1)\n",
    "evaluation_df = query_results.merge(pd.read_csv('gpt_synonym_df.csv'),left_on='query_name',right_on='synonym')\n",
    "evaluation_df['correct_match'] = evaluation_df['hp_id_x'] == evaluation_df['hp_id_y']\n",
    "# rate of correct_match\n",
    "evaluation_df['correct_match'].sum()/evaluation_df['correct_match'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hpo single typo evaluation\n",
    "import re\n",
    "\n",
    "input_json = json.load(open('./single_typo.json'))\n",
    "query_name_id_list = []\n",
    "for i in input_json:\n",
    "    # The Human Phenotype Ontology term Multicystic dysplastic kidney is identified by the HPO ID\n",
    "    # re match\n",
    "    m1 = re.match('The Human Phenotype Ontology term (.+?) is identified by the HPO ID', i['input'])\n",
    "    m2 = re.match('The HPO term (.+?) represents', i['input'])\n",
    "    m3 = re.match('The HPO ID of (.+?) corresponds to', i['input'])\n",
    "    m4 = re.match('The HPO ID of (.+?) is', i['input'])\n",
    "    m5 = re.match('The HPO ID of the concept (.+?) is', i['input'])\n",
    "\n",
    "    if m1 or m2 or m3 or m4:\n",
    "        if m1 is not None:\n",
    "            query_name_id_list.append({'query_term':m1.group(1),'hp_id':i['output']})\n",
    "        if m2 is not None:\n",
    "            query_name_id_list.append({'query_term':m2.group(1),'hp_id':i['output']})\n",
    "        if m3 is not None:\n",
    "            query_name_id_list.append({'query_term':m3.group(1),'hp_id':i['output']})\n",
    "        if m5 is not None:\n",
    "            query_name_id_list.append({'query_term':m5.group(1),'hp_id':i['output']})\n",
    "        else:\n",
    "            if m4 is not None:\n",
    "                query_name_id_list.append({'query_term':m4.group(1),'hp_id':i['output']})\n",
    "    else:\n",
    "        print(i['input'])\n",
    "\n",
    "query_name_id_list_df = pd.DataFrame(query_name_id_list)\n",
    "query_name_id_list_df['hp_id'] = query_name_id_list_df['hp_id'].apply(lambda x: x.replace(':','_'))\n",
    "query_name_id_list_df = query_name_id_list_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17938 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 521/17938 [00:19<10:50, 26.76it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m index_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./woosh_index\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m query_name_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(query_name_id_list_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery_term\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()))\n\u001b[0;32m----> 4\u001b[0m query_results \u001b[38;5;241m=\u001b[39m \u001b[43mquery_terms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_name_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 76\u001b[0m, in \u001b[0;36mquery_terms\u001b[0;34m(index_dir, query_name_list, output, top_k)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query_name \u001b[38;5;129;01min\u001b[39;00m tqdm(query_name_list):\n\u001b[1;32m     75\u001b[0m     query \u001b[38;5;241m=\u001b[39m query_parser\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(query_name))\n\u001b[0;32m---> 76\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43msearcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscored\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# Retrieve the top k matching phrase\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(results) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;66;03m# Retrieve and print the ranked matching phrases\u001b[39;00m\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;66;03m# top k only\u001b[39;00m\n",
      "File \u001b[0;32m/phi_home/cl3720/phi/RESCUE/RARE-GPT/.openai/lib/python3.10/site-packages/whoosh/searching.py:786\u001b[0m, in \u001b[0;36mSearcher.search\u001b[0;34m(self, q, **kwargs)\u001b[0m\n\u001b[1;32m    784\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollector(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    785\u001b[0m \u001b[38;5;66;03m# Call the lower-level method to run the collector\u001b[39;00m\n\u001b[0;32m--> 786\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_with_collector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;66;03m# Return the results object from the collector\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c\u001b[38;5;241m.\u001b[39mresults()\n",
      "File \u001b[0;32m/phi_home/cl3720/phi/RESCUE/RARE-GPT/.openai/lib/python3.10/site-packages/whoosh/searching.py:819\u001b[0m, in \u001b[0;36mSearcher.search_with_collector\u001b[0;34m(self, q, collector, context)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Allow collector to set up based on the top-level information\u001b[39;00m\n\u001b[1;32m    817\u001b[0m collector\u001b[38;5;241m.\u001b[39mprepare(\u001b[38;5;28mself\u001b[39m, q, context)\n\u001b[0;32m--> 819\u001b[0m \u001b[43mcollector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/phi_home/cl3720/phi/RESCUE/RARE-GPT/.openai/lib/python3.10/site-packages/whoosh/collectors.py:144\u001b[0m, in \u001b[0;36mCollector.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subsearcher, offset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_searcher\u001b[38;5;241m.\u001b[39mleaf_searchers():\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_subsearcher(subsearcher, offset)\n\u001b[0;32m--> 144\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_matches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m/phi_home/cl3720/phi/RESCUE/RARE-GPT/.openai/lib/python3.10/site-packages/whoosh/collectors.py:214\u001b[0m, in \u001b[0;36mCollector.collect_matches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"This method calls :meth:`Collector.matches` and then for each\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03mmatched document calls :meth:`Collector.collect`. Sub-classes that\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03mwant to intervene between finding matches and adding them to the\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03mcollection (for example, to filter out certain documents) can override\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03mthis method.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    213\u001b[0m collect \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sub_docnum \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatches():\n\u001b[1;32m    215\u001b[0m     collect(sub_docnum)\n",
      "File \u001b[0;32m/phi_home/cl3720/phi/RESCUE/RARE-GPT/.openai/lib/python3.10/site-packages/whoosh/collectors.py:409\u001b[0m, in \u001b[0;36mScoredCollector.matches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m# If we're using block quality optimizations, and the checkquality\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;66;03m# flag is true, try to skip ahead to the next block with the\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;66;03m# minimum required quality\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m usequality \u001b[38;5;129;01mand\u001b[39;00m checkquality \u001b[38;5;129;01mand\u001b[39;00m minscore \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipped_times \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mmatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskip_to_quality\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminscore\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;66;03m# Skipping ahead might have moved the matcher to the end of the\u001b[39;00m\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;66;03m# posting list\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m matcher\u001b[38;5;241m.\u001b[39mis_active():\n",
      "File \u001b[0;32m/phi_home/cl3720/phi/RESCUE/RARE-GPT/.openai/lib/python3.10/site-packages/whoosh/matching/binary.py:294\u001b[0m, in \u001b[0;36mUnionMatcher.skip_to_quality\u001b[0;34m(self, minquality)\u001b[0m\n\u001b[1;32m    292\u001b[0m         aq \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mblock_quality()\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 294\u001b[0m         skipped \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskip_to_quality\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminquality\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m         bq \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mblock_quality()\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m skipped\n",
      "File \u001b[0;32m/phi_home/cl3720/phi/RESCUE/RARE-GPT/.openai/lib/python3.10/site-packages/whoosh/codec/whoosh3.py:1034\u001b[0m, in \u001b[0;36mW3LeafMatcher.skip_to_quality\u001b[0;34m(self, minquality)\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_active() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid() \u001b[38;5;241m<\u001b[39m targetid:\n\u001b[1;32m   1032\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext()\n\u001b[0;32m-> 1034\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mskip_to_quality\u001b[39m(\u001b[38;5;28mself\u001b[39m, minquality):\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;66;03m# Skip blocks until we find one that might exceed the given minimum\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# quality\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m     block_quality \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_quality\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;66;03m# If the quality of this block is already higher than the minimum,\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;66;03m# do nothing\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output = './woosh_hpo_single_tpypo_query_results.csv'\n",
    "index_dir = './woosh_index'\n",
    "query_name_list = list(set(query_name_id_list_df['query_term'].tolist()))\n",
    "query_results = query_terms(index_dir, query_name_list, output, top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40480961923847697"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_df = query_results.merge(query_name_id_list_df,left_on='query_name',right_on='query_term')\n",
    "evaluation_df['correct_match'] = evaluation_df['hp_id_x'] == evaluation_df['hp_id_y']\n",
    "# rate of correct_match\n",
    "evaluation_df['correct_match'].sum()/evaluation_df['correct_match'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
